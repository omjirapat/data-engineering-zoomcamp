{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c42c37-4cc6-4aab-8146-91bbfe1772a4",
   "metadata": {},
   "source": [
    "Question 1. Redpanda version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ac42c-1216-40cc-8e8d-a641269bfb25",
   "metadata": {},
   "source": [
    "Now let's find out the version of redpandas.\n",
    "\n",
    "For that, check the output of the command rpk help inside the container. The name of the container is redpanda-1.\n",
    "\n",
    "Find out what you need to execute based on the help output.\n",
    "\n",
    "What's the version, based on the output of the command you executed? (copy the entire version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c37e21-8d09-41ab-bc5b-70801ada7158",
   "metadata": {},
   "source": [
    "```\n",
    "docker exec -it redpanda-1 bash -c \"rpk version\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb510e0-6bed-43fb-9484-26c542c9d088",
   "metadata": {},
   "source": [
    "output : v22.3.5 (rev 28b2443)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d54af9-1886-46da-877b-cda90b4ecc32",
   "metadata": {},
   "source": [
    "Question 2. Creating a topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971f7728-247e-40ec-9776-4727324dca87",
   "metadata": {},
   "source": [
    "Before we can send data to the redpanda server, we need to create a topic. We do it also with the rpk command we used previously for figuring out the version of redpandas.\n",
    "\n",
    "Read the output of help and based on it, create a topic with name test-topic\n",
    "\n",
    "What's the output of the command for creating a topic? Include the entire output in your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706f3bfc-c848-4820-a3db-7c88d7dff0b1",
   "metadata": {},
   "source": [
    "```\n",
    " docker exec -it redpanda-1 bash -c \"rpk topic create test-topic\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fcaec8-9767-46f3-a764-b88fa8fabce8",
   "metadata": {},
   "source": [
    "Question 3. Connecting to the Kafka server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d0b438-1474-4586-a624-0d04f2380a0c",
   "metadata": {},
   "source": [
    "We need to make sure we can connect to the server, so later we can send some data to its topics\n",
    "Provided that you can connect to the server, what's the output of the last command?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a528c9-ce46-42fa-aaf7-86db9d8988ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in /usr/local/python/3.9.18/lib/python3.9/site-packages (2.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b033065d-7e57-494a-9887-43de69f652fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import time \n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "\n",
    "server = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[server],\n",
    "    value_serializer=json_serializer\n",
    ")\n",
    "\n",
    "producer.bootstrap_connected()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182a8cf8-a9d4-4302-a7b2-b391c6637627",
   "metadata": {},
   "source": [
    "Question 4. Sending data to the stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021dcafd-2a9e-410f-8552-330eb84e65c2",
   "metadata": {},
   "source": [
    "Now we're ready to send some test data:\n",
    "Where did it spend most of the time? → Sending the messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5972070a-87b9-45ca-9eda-01b150df9778",
   "metadata": {},
   "source": [
    "Question 5. Time to send data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956159a1-593e-41f8-859e-e14e9cd73ceb",
   "metadata": {},
   "source": [
    "→ \n",
    "How much time in seconds did it take? (You can round it to a whole number)→ \r\n",
    "Make sure you don't include sleeps in your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab9f8274-6a1d-429d-813a-6bd199144ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: {'number': 0}\n",
      "Sent: {'number': 1}\n",
      "Sent: {'number': 2}\n",
      "Sent: {'number': 3}\n",
      "Sent: {'number': 4}\n",
      "Sent: {'number': 5}\n",
      "Sent: {'number': 6}\n",
      "Sent: {'number': 7}\n",
      "Sent: {'number': 8}\n",
      "Sent: {'number': 9}\n",
      "took 0.52 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "topic_name = 'test-topic'\n",
    "\n",
    "for i in range(10):\n",
    "    message = {'number': i}\n",
    "    producer.send(topic_name, value=message)\n",
    "    print(f\"Sent: {message}\")\n",
    "    time.sleep(0.05)\n",
    "\n",
    "producer.flush()\n",
    "\n",
    "t1 = time.time()\n",
    "print(f'took {(t1 - t0):.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7eb095-37ca-48cf-aa21-069a6cf8f8a0",
   "metadata": {},
   "source": [
    "Question 6. Parsing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2ea103-4f7b-455f-a90c-fe560d84b554",
   "metadata": {},
   "source": [
    "→ Create a topic green-trips and send the data there\n",
    "How does the record look after parsing? Copy the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a6a2269-c347-48c0-ad86-0a4be6b83592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 45.54 seconds\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "col=['lpep_pickup_datetime', 'lpep_dropoff_datetime', 'PULocationID', \n",
    "        'DOLocationID', 'passenger_count', 'trip_distance', 'tip_amount']\n",
    "df_green = pd.read_csv('green_tripdata_2019-10.csv',index_col=False,usecols=col)\n",
    "t0 = time.time()\n",
    "topic_name = 'green-trips'\n",
    "for row in df_green.itertuples(index=False):\n",
    "    row_dict = {col: getattr(row, col) for col in row._fields}\n",
    "    producer.send(topic_name, value=row_dict)\n",
    "\n",
    "producer.flush()\n",
    "\n",
    "t1 = time.time()\n",
    "print(f'took {(t1 - t0):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7615616-ae02-44f2-ae26-9f225c4a1892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/sdkman/candidates/spark/3.5.0/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/vscode/.ivy2/cache\n",
      "The jars for the packages stored in: /home/vscode/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f7e7bee1-1a5f-454c-a391-5e3967f3e08a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 277ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f7e7bee1-1a5f-454c-a391-5e3967f3e08a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/5ms)\n",
      "24/03/19 15:45:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "pyspark_version = pyspark.__version__\n",
    "kafka_jar_package = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{pyspark_version}\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GreenTripsConsumer\") \\\n",
    "    .config(\"spark.jars.packages\", kafka_jar_package) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "991080f0-c335-46e8-9f2a-6874be3f8e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"green-trips\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a928eef-c294-4bf5-a2d2-43d5ca02a9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/19 15:45:52 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-3ed3b518-9927-4484-a82d-cc8a44cb710b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/19 15:45:52 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/03/19 15:45:53 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(key=None, value=bytearray(b'{\"lpep_pickup_datetime\": \"2019-10-01 00:26:02\", \"lpep_dropoff_datetime\": \"2019-10-01 00:39:58\", \"PULocationID\": 112, \"DOLocationID\": 196, \"passenger_count\": 1.0, \"trip_distance\": 5.88, \"tip_amount\": 0.0}'), topic='green-trips', partition=0, offset=0, timestamp=datetime.datetime(2024, 3, 19, 12, 11, 24, 344000), timestampType=0)\n"
     ]
    }
   ],
   "source": [
    "def peek(mini_batch, batch_id):\n",
    "    first_row = mini_batch.take(1)\n",
    "\n",
    "    if first_row:\n",
    "        print(first_row[0])\n",
    "\n",
    "query = green_stream.writeStream.foreachBatch(peek).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aac3049-5934-40d2-892e-d289cef81755",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73902bfe-067f-4dac-ba35-b9b3b158cef3",
   "metadata": {},
   "source": [
    "Question 7. Most popular destination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b3fb95-9331-4925-be9a-f009ea8afd47",
   "metadata": {},
   "source": [
    "Now let's finally do some streaming analytics. We will see what's the most popular destination currently based on our stream of data (which ideally we should have sent with delays like we did in workshop 2)\r\n",
    "\r\n",
    "This is how you can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5f72c89-8a1c-4bb3-af01-0a048d82ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "schema = types.StructType() \\\n",
    "    .add(\"lpep_pickup_datetime\", types.StringType()) \\\n",
    "    .add(\"lpep_dropoff_datetime\", types.StringType()) \\\n",
    "    .add(\"PULocationID\", types.IntegerType()) \\\n",
    "    .add(\"DOLocationID\", types.IntegerType()) \\\n",
    "    .add(\"passenger_count\", types.DoubleType()) \\\n",
    "    .add(\"trip_distance\", types.DoubleType()) \\\n",
    "    .add(\"tip_amount\", types.DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2415bc36-7a4c-4312-9274-3b60945f00b2",
   "metadata": {},
   "source": [
    "→ Add a column \"timestamp\" using the current_timestamp function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b617e49-7b4b-434a-a270-d76950881ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "green_stream = (\n",
    "    green_stream\n",
    "    .select(F.from_json(F.col(\"value\").cast('STRING'), schema).alias(\"data\"))\n",
    "    .select(\"data.*\")\n",
    "    .withColumn('timestamp', F.current_timestamp())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3b9f9d-8f69-47e6-9724-6e7cc737f227",
   "metadata": {},
   "source": [
    "Group by:\r",
    "→ \n",
    "5 minutes window based on the timestamp column (F.window(col(\"timestamp\"), \"5 minutes\"))→ \r\n",
    "\"DOLocationI\n",
    "Order by count\n",
    "Write the most popular destination, your answer should be either the zone ID or the zone name of this destination. (You will need to re-send the data for this to work)D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "971d8309-3a6b-463f-a6e6-a23faa11ae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_destinations = (\n",
    "    green_stream\n",
    "    .groupBy([F.window(\"timestamp\", \"5 minutes\"), \"DOLocationID\"])\n",
    "    .count().alias(\"count\")\n",
    "    .sort(\"count\", ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86f2cde-aaa8-4b5f-90b3-32890c08058d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/19 15:46:10 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-fe9d35f3-d04a-486a-8f59-4aa2e9afef63. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/19 15:46:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/03/19 15:46:10 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------------------------------------------+------------+-----+\n",
      "|window                                    |DOLocationID|count|\n",
      "+------------------------------------------+------------+-----+\n",
      "|{2024-03-19 15:45:00, 2024-03-19 15:50:00}|74          |35482|\n",
      "|{2024-03-19 15:45:00, 2024-03-19 15:50:00}|42          |31884|\n",
      "|{2024-03-19 15:45:00, 2024-03-19 15:50:00}|41          |28122|\n",
      "|{2024-03-19 15:45:00, 2024-03-19 15:50:00}|75          |25680|\n",
      "|{2024-03-19 15:45:00, 2024-03-19 15:50:00}|129         |23860|\n",
      "|{2024-03-19 15:45:00, 2024-03-19 15:50:00}|7           |23066|\n",
      "|{2024-03-19 15:45:00, 2024-03-19 15:50:00}|166         |21690|\n",
      "|{2024-03-19 15:45:00, 2024-03-19 15:50:00}|236         |15826|\n",
      "|{2024-03-19 15:45:00, 2024-03-19 15:50:00}|223         |15084|\n",
      "|{2024-03-19 15:45:00, 2024-03-19 15:50:00}|238         |14636|\n",
      "|{2024-03-19 15:45:00, 2024-03-19 15:50:00}|82          |14584|\n",
      "|{2024-03-19 15:45:00, 2024-03-19 15:50:00}|181         |14564|\n",
      "|{2024-03-19 15:45:00, 2024-03-19 15:50:00}|95          |14488|\n",
      "|{2024-03-19 15:45:00, 2024-03-19 15:50:00}|244         |13466|\n",
      "|{2024-03-19 15:45:00, 2024-03-19 15:50:00}|61          |13212|\n",
      "|{2024-03-19 15:45:00, 2024-03-19 15:50:00}|116         |12678|\n",
      "|{2024-03-19 15:45:00, 2024-03-19 15:50:00}|138         |12288|\n",
      "|{2024-03-19 15:45:00, 2024-03-19 15:50:00}|97          |12100|\n",
      "|{2024-03-19 15:45:00, 2024-03-19 15:50:00}|49          |10442|\n",
      "|{2024-03-19 15:45:00, 2024-03-19 15:50:00}|151         |10306|\n",
      "+------------------------------------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = popular_destinations \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
